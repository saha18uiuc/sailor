{
	"global_batch_size": 2048,
	"type": "gpt2",
	"hidden_size": 4096,
	"sequence_length": 8192,
	"num_layers": 32,
	"vocab_size": 128256,
	"model": "LLAMA-3-8",
	"optimizer": "Adam",
	"heads": 32,
	"head_dim": 128,
	"max_position_embeddings": 8192,
	"num_all_layers": 35
  }